---
title:  "Deep Learning para crear música clásica"
date: 2018-10-28
tags: [deep-learning]
excerpt: Generating classical music with Deep Learning
---

Hace unos días leíamos [en
prensa](https://www.xataka.com/robotica-e-ia/este-cuadro-ha-pintado-maquina-alguien-ha-comprado-432-500-dolares-christies)
que, por primera vez, un cuadro generado por una IA había sido vendido
en una subasta por más de 400 000 dólares. Aunque este tipo de redes
neuronales, capaces de generar contenido [existen desde hace unos
años](https://arxiv.org/abs/1406.2661), esta compra ha tenido una gran
repercusión, mostrando al público los impactantes resultados que
podemos conseguir con el uso de modelos de Deep Learning


**Por cierto...** A raíz de esta noticia, surgió una polémica porque
parece que los creadores de este cuadro se limitaron a copiar la red
de otro investigador. El otro día [hablé en Twitter del
tema](https://twitter.com/unmonoqueteclea/status/1056632957816000515)


![Cuadro IA](/assets/images/posts/cuadroIA.jpg){:class="img-responsive center-image"}

**Importante** Este artículo está inspirado por [este otro publicado
en
towardsdatascience](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5)
y prácticamente la totalidad del código procede del mismo.


Hoy veremos que cualquiera puede, desde casa y con un equipo mediocre,
crear un modelo que sea capaz de crear arte. En este caso, nos
centraremos en algo no tan explotado, la **generación de piezas
musicales**. Como veremos, existe un tipo de redes neuronales, las
**LSTM (Long Short-Term Memory)** con unas características que las
hacen ideales para esta tarea.

Entrenaremos las LSTM con unos archivos MIDI con **50 composiciones de
Chopin**, y estas serán capaces, tras el entrenamiento, de componer
nuevas obras. Para la implementación, usaremos
[Keras](https://keras.io/), y la realizaremos en una [Jupyter
Notebook](http://jupyter.org/) en [Google
Colaboratory](https://colab.research.google.com/).

**Show me the code!** En este post únicamente se mostrarán algunas
líneas de código representativas. El código completo está en [Google
Colab](https://colab.research.google.com/drive/19TQqekOlnOSW36VCL8CPVEQKBBukmaEQ)
y en
[github](https://github.com/unmonoqueteclea/DeepLearning-Notebooks/tree/master/LSTM-Music-Generation)
{:.notice--info}


Empecemos por el final. Vamos a comenzar viendo las composiciones que
nuestra red neuronal ha realizado.  Suenan bien ¿no?

<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/520806525&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true"></iframe>


<iframe width="100%" height="166" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/520806507&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true"></iframe>


## Redes Neuronales Recurrentes y LSTM
Las **Redes Neuronales Recurrentes (RNN)** son un tipo de redes
neuronales que integran bucles de realimentación Esta realimentación
hace que las RNN sean ideales para trabajar con secuencias o
listas. Para quien quiera saber más de este tipo de redes, recomiendo
este [interesantísimo
post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

Como vemos en [este otro famoso
post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/),
podemos imaginar una RNN como copias de la misma red, pasando cada una
un mensaje a su sucesora.

![RNN vs DNN t](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png){:class="img-responsive center-image"}

**El problema con este tipo de redes viene cuando tenemos que tratar
dependencias a largo plazo**. Aunque teóricamente este tipo de redes
serían capaces de trabajar con este tipo de dependencias, varios
investigadores han demostrado que, en la práctica, es bastante
dificil. Para solventar este problema existen las **redes LSTM (Long
Short-Term Memory)**. Este tipo de redes recurrentes son capaces de
aprender este tipo de dependencias puesto que están diseñadas para
recordar la información durante largos periodos.


## Representando música
Para entrenar nuestro modelo usaremos [ficheros
MIDI](https://en.wikipedia.org/wiki/MIDI). Este tipo de ficheros
contienen **toda la información sobre una composición musical**, pero
no la propia música. Es decir, contienen información, no audio. La
información contenida es el instrumento, la notación, altura de cada
nota, duración, paneo, etc. Existe software de síntesis capaz de crear
pistas de audio a partir de estos ficheros MIDI. Nuestro modelo
aprenderá de ellos, y tras el entrenamiento, será capaz de generar
nuevos.

![MIDI file visualization](http://i1-win.softpedia-static.com/screenshots/Speedy-MIDI_1.png){:class="img-responsive center-image"}

## music21
En vez de usar los ficheros MIDI en bruto, que serían dificil de
comprender por la red, vamos a aprovecharnos de la representación que
hace de estos el paquete [music21 de
Python](http://web.mit.edu/music21/).  Este paquete contiene una serie
de herramientas que nos permiten trabajar con ficheros MIDI de forma
sencilla puesto que crea una representación de cada fichero con
secuencias formadas por objetos de tipo **Note** o **Chord**. Es una
representación más fácil de comprender para la red neuronal


## Obteniendo ficheros MIDI
Para entrenar la red, hemos elegido 50 obras para piano de Chopin
obtenidas de [esta web](http://www.piano-midi.de/). Para el buen
funcionamiento de nuestro programa, vamos a usar las versiones en
**MIDI Format 0**, que podremos obtener en [este
enlace](http://www.piano-midi.de/midis/format0/). La diferencia de
este formato con el del resto de pistas que podemos encontrar en esta
misma web es que, en en los ficheros MIDI con Format 0, todas las
voces están en el mismo canal MIDI.


## Procesando la información
Ya tenemos todas las canciones para entrenar nuestra red. Ahora hay
que procesar estos ficheros para darle un formato a la red.

En este primer fragmento de código, estamos leyendo todos los
elementos de las 50 obras, y añadiéndolos a un mismo vector. Tenemos dos
tipos de elementos que podemos encontrar: notas o acordes.

- En el caso de las notas, la representación que obtenemos es la formada
por el nombre de la nota + la octava.

- En el caso de los acordes, la representación consiste en una serie de
números separados por puntos. Cada uno de estos númetros, representa
una nota en concreto.

```python
notes = []
for i,file in enumerate(glob.glob("midi_files/chopin/*.mid")):
  midi = converter.parse(file)
  print('\r', 'Parsing file ', i, " ",file, end='')
  notes_to_parse = None
  try: # file has instrument parts
    s2 = instrument.partitionByInstrument(midi)
    notes_to_parse = s2.parts[0].recurse()
  except: # file has notes in a flat structure
    notes_to_parse = midi.flat.notes
  for element in notes_to_parse:
    if isinstance(element, note.Note):
      notes.append(str(element.pitch))
    elif isinstance(element, chord.Chord):
      notes.append('.'.join(str(n) for n in element.normalOrder))
with open('notes', 'wb') as filepath:
  pickle.dump(notes, filepath)
```

Usamos [pickle](https://docs.python.org/3/library/pickle.html) para no tener
que realizar todo el procesamiento cada vez que queramos ejecutar el
notebook.

## Generando secuencias mediante clasificación
Tras esto, ya tenemos el contenido de todas las obras en un solo
vector.  Sin embargo, nuestra información aún no tiene el formato
necesario para entrenar la red. Cuando aprendemos Machine Learning,
aprendemos dos tipos de aprendizaje: **Aprendizaje Supervisado y Aprendizaje no
Supervisado**. Aprendemos también que dentro del aprendizaje supervisado
tenemos problemas de **clasificación** (predecir un valor discreto) y
problemas de **regresión** (predecir un valor continuo). Sin embargo,
¿Cuál es el tipo de aprendizaje para generar una secuencia de
valores?. **Esto no es más que un problema de clasificación, el
*truco* está en cómo le pasamos los datos**.

Los inputs de la red son secuencias de un determinado número de notas
y acordes. El output de la red será la siguiente nota o acorde. De
esta forma, iremos pasando a la red, de forma iterativa, el estado actual de la
pieza para que la red ésta una predicción de cuál debe ser la
siguiente nota.

Veamos un diagrama de todo esto:
![Generating
sequences](/assets/images/posts/inputoutputsequences.png){:class="img-responsive
center-image"}

## Creando el modelo
Ahora sí, ya podemos crear el modelo.  Nuestra red tendrá dos capas de
tipo **LSTM**, 3 capas de tipo **Dropout**, y otras 2 **Fully
Connected**. Cada capa tiene una función específica.

- Las **LSTM**, como hemos comentado, son las que permiten aprender
secuencias.
- Las 3 capas de **Dropout**, nos permitirán evitar el **overfiting**,
es decir, que la red no sea capaz de generalizar y simplemente imite
ejemplos con los que ha sido entrenada. Precisamente, en este tipo de
modelos el **overfiting** es muy común, y habrá que tomar medidas para
evitarlo. La buena noticia es que es fácilmente detectable sin
necesidad de usar un dataset de validación. Si la obra generada (ya
sea un texto, imagen o audio), es prácticamente igual que una de las
usadas para el entrenamiento, nuestra red no está siendo capaz de
generalizar.

**¡OJO!** De hecho, tengo la sospecha de que esto mismo es lo que está
pasando en nuestra red, aunque aún no he encontrado qué fragmento de las
obras de Chopin es el que está imitando.
{: .notice--info}

Este es el código que usaremos para la creación del modelo:
```python
def create_network(network_input, n_vocab):
    """ create the structure of the neural network """
    model = Sequential()
    model.add(LSTM(
        512,
        input_shape=(network_input.shape[1], network_input.shape[2]),
        return_sequences=True
    ))
    model.add(Dropout(0.3))
    model.add(LSTM(512, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(512))
    model.add(Dense(256))
    model.add(Dropout(0.3))
    model.add(Dense(n_vocab))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
    return model
```

## Generando música
Ya hemos preparado los datos y preparado el modelo. También hemos
entrenado el mismo durante varias horas (o días). Respecto al entrenamiento,
en este caso se ha usado un **batch_size** de 64, y se ha entrenado durante
13 horas en Google Colab (75 epochs).

Es el momento de componer nuestra propia música. Los pasos para
realizarlo serán los siguientes:
- Tomamos una secuencia de 100 notas de alguna de nuestras obras de
  entrenamiento y la usamos como input a nuestro modelo.
- Generamos una predicción de la siguiente nota
- La añadimos al final de nuestra secuencia de inputs de la red y al
  vector que representa nuestra obra final
- De la secuencia de inputs de nuestra red, quitamos el elemento más antiguo.
- Repetimos todos los pasos anteriores hasta haber generado un
  determinado número de elementos nuevos.

Como una imagen vale más que mil palabras, vamos a ver un ejemplo del proceso.

![Generating
sequences](/assets/images/posts/lstm.png){:class="img-responsive
center-image"}


```python
# Pick a random sequence from the input as a starting
# point for the prediction
start = numpy.random.randint(0, len(network_input)-1)
int_to_note = dict((number, note) for number, note in enumerate(pitchnames))
pattern = network_input[start]
prediction_output = []
# Generate 500 notes
for i,note_index in enumerate(range(500)):
  prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))
  prediction_input = prediction_input / float(n_vocab)
  prediction = model.predict(prediction_input, verbose=0)
  index = numpy.argmax(prediction)
  result = int_to_note[index]
  print('\r', 'Predicted ', i, " ",result, end='')
  prediction_output.append(result)
  pattern.append(index)
  pattern = pattern[1:len(pattern)]
```

Tras esto, simplemente tenemos que realizar el proceso contrario al que
hemos realizado al principio. A partir del vector de objetos de
**music21**, podremos crear un fichero MIDI.

```python
offset = 0
output_notes = []
# create note and chord objects based on the values generated by the model
for pattern in prediction_output:
    # pattern is a chord
    if ('.' in pattern) or pattern.isdigit():
        notes_in_chord = pattern.split('.')
        notes = []
        for current_note in notes_in_chord:
            new_note = note.Note(int(current_note))
            new_note.storedInstrument = instrument.Piano()
            notes.append(new_note)
        new_chord = chord.Chord(notes)
        new_chord.offset = offset
        output_notes.append(new_chord)
    # pattern is a note
    else:
        new_note = note.Note(pattern)
        new_note.offset = offset
        new_note.storedInstrument = instrument.Piano()
        output_notes.append(new_note)

    # increase offset each iteration so that notes do not stack
    offset += 0.5

midi_stream = stream.Stream(output_notes)
midi_stream.write('midi', fp='test_output.mid')
```

## Consideraciones finales
Como vemos, hemos sido capaces de generar dos canciones con bastante
sentido.  Para ello, no hemos necesitado ni siquiera tener una GPU,
puesto que nos hemos aprovechado de las ventajas que nos ofrece
**Google Colaboratory**.

Para escuchar vuestros propios ficheros MIDIs tenéis varias opciones.
Una de ellas, es usar cualquier secuenciador online de MIDIs como
[este](https://onlinesequencer.net). Otra opción, un poco más
compleja, es usar editores de audio que permitan añadir pistas MIDI
como [Ableton](https://www.ableton.com/en/) o, el que yo he usado y
recomiendo, [Ardour](https://ardour.org/).

Hay mucho trabajo por delante, y mucho margen de mejora para el
programa. El principal inconveniente de la mayoría de mejoras, es que
suponenen un incremento de la complejidad de la red, y por lo tanto,
más consumo de memoria y más tiempo de entrenamiento
necesario. Algunas de estas son:

- Aprender, además de la altura de las notas, su duración. Esto supondría
  que nuestra red tendría que aprender muchas más clases distintas, que
  serían combinación de una altura y una duración determinadas.
- Intentar crear un ejemplo con dos instrumentos distintos. Por
  ejemplo, podría intentar aprender piezas de piano y voz.

Mi recomendación es que no os limitéis a leer este post, sino que os
descarguéis [el
código](https://github.com/unmonoqueteclea/DeepLearning-Notebooks/tree/master/LSTM-Music-Generation)
y empecéis a jugar con él. Se puede entrenar con otros autores, con
otros estilos musicales, probar distintos *sequence length*,...

<style> .center-image{ margin: 0 auto; display: block; } </style>
